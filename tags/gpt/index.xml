<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>gpt on Aziz et al.</title><link>https://azayz.github.io/tags/gpt/</link><description>Recent content in gpt on Aziz et al.</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sat, 01 Mar 2025 02:01:58 +0530</lastBuildDate><atom:link href="https://azayz.github.io/tags/gpt/index.xml" rel="self" type="application/rss+xml"/><item><title>GPT2 From Scratch For Dummies!</title><link>https://azayz.github.io/posts/gpt-2-from-scratch-for-dummies/</link><pubDate>Sat, 01 Mar 2025 02:01:58 +0530</pubDate><guid>https://azayz.github.io/posts/gpt-2-from-scratch-for-dummies/</guid><description>GPT-2 Implementation From Scratch For Dummies! If you ever had trouble understanding the code behind the attention and the research paper&amp;rsquo;s complex code, this is for you.
In this post, I break down the inner workings of a GPT-2 modelâ€”from token embeddings and positional encodings to multi-head self-attention and MLP blocks. I&amp;rsquo;ll try to &amp;ldquo;dumb&amp;rdquo; things down and explain the details and intution behind the each line of the code as much as I can, if you&amp;rsquo;re a dummy like me, I hope this helps.</description></item></channel></rss>