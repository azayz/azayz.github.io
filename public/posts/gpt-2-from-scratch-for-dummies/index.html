<!DOCTYPE html>
<html><head lang="en"><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge"><title>GPT2 From Scratch For Dummies! - Aziz et al.</title><meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="description" content="Implementing the GPT2 architecture from scratch in Pytorch but going slow and as intuitive as possible so we can soak in all the details." />
	<meta property="og:image" content=""/>
	<meta property="og:url" content="http://localhost:1313/posts/gpt-2-from-scratch-for-dummies/">
  <meta property="og:site_name" content="Aziz et al.">
  <meta property="og:title" content="GPT2 From Scratch For Dummies!">
  <meta property="og:description" content="Implementing the GPT2 architecture from scratch in Pytorch but going slow and as intuitive as possible so we can soak in all the details.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-03-01T02:01:58+05:30">
    <meta property="article:modified_time" content="2025-03-01T02:01:58+05:30">
    <meta property="article:tag" content="Gpt">
    <meta property="article:tag" content="Dummies">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="GPT2 From Scratch For Dummies!">
  <meta name="twitter:description" content="Implementing the GPT2 architecture from scratch in Pytorch but going slow and as intuitive as possible so we can soak in all the details.">

        <link href="http://localhost:1313/css/fonts.2c2227b81b1970a03e760aa2e6121cd01f87c88586803cbb282aa224720a765f.css" rel="stylesheet">
	

	
	<link rel="stylesheet" type="text/css" media="screen" href="http://localhost:1313/css/main.6a0f23ea50fd34b46fee262a5a68e17d458c51a2bc99ba1ba018065de6b180c3.css" />
		<link id="darkModeStyle" rel="stylesheet" type="text/css" href="http://localhost:1313/css/dark.50b57e12d401420df23965fed157368aba37b76df0ecefd0b1ecd4da664f01a0.css"   />
</head>
<body>
        <div class="content"><header>
	<div class="main">
		<a href="http://localhost:1313/">Aziz et al.</a>
	</div>
	<nav>
		
		<a href="/">Home</a>
		
		<a href="/posts">All posts</a>
		
		<a href="/about">About</a>
		
		<a href="/tags">Tags</a>
		
		
	</nav>
</header>

<main>
  <article>
    <div class="post-container">
      
      <div class="post-content">
        <div class="title">
          <h1 class="title">GPT2 From Scratch For Dummies!</h1>
          <div class="meta">Posted on Mar 1, 2025</div>
        </div>
        
        <section class="body">
          <h1 id="gpt-2-implementation-from-scratch-for-dummies">GPT-2 Implementation From Scratch For Dummies!</h1>
<p>If you ever had trouble understanding the code behind the attention and the research paper&rsquo;s complex code, this is for you.</p>
<p>In this post, I break down the inner workings of a GPT-2 modelâ€”from token embeddings and positional encodings to multi-head self-attention and MLP blocks.
I&rsquo;ll try to &ldquo;dumb&rdquo; things down and explain the details and intution behind the each line of the code as much as I can,
if you&rsquo;re a dummy like me, I hope this helps.</p>
<h2 id="1-input--token-embeddings">1. Input &amp; Token Embeddings</h2>
<p>The input to LLMs is a batch of sequences, so usually the input is a matrix of shape <code>(batch_size B, sequence_length T)</code>.</p>
<p>B represents the batch size, T represents the number of tokens in a specific sequence.</p>
<h2 id="2-token-embedding-layer">2. Token Embedding Layer</h2>
<p>First layer is the token embedding layer with the shape of <code>(vocab_size V, hidden_size C)</code>:</p>
<ul>
<li><code>vocab_size</code>: Total number of tokens in our vocabulary</li>
<li><code>hidden_size</code> or <code>n_embd</code>: number of dimension we want to embed each to token to. For example if hidden_size = 768
then each token would be represented with a vector of 768 entries.</li>
</ul>
<h2 id="3-token-embedding-output">3. Token Embedding Output</h2>
<p>After going through the token embedding layer (This operation is not matrix multiplication, it&rsquo;s more of a look up table),
the resulting output has the shape <code>(batch_size B, sequence_length T, hidden_size C)</code>.</p>
<p>This happens because now we go through the batches, in every batch we have a sequence of tokens with tokens being numbers
ranging from 0 to vocab_size - 1 and now we replace each token with a vector of size hidden_size C.</p>
<h2 id="4-positional-encoding">4. Positional Encoding</h2>
<p>Next layer is the positional encoding layer with the shape of <code>(sequence_length T, hidden_size C)</code>.</p>
<p>This means that each position in the sequence will have an embedding representation to give it meaning.</p>
<p>No need to pass any input through the position embedding, it is used to be added after the input comes out from word token embedding layer.</p>
<p>So now we have a matrix with shape of <code>(batch_size B, sequence_length T, hidden_size C)</code> and for each batch size we add the positional embeddings matrix which is <code>(sequence_length T, hidden_size C)</code>.</p>
<h2 id="5-transformer-blocks">5. Transformer Blocks</h2>
<p>Now this input goes through N number of blocks, each block contains layers that do transformations.</p>
<p>A block structure has the following layers:</p>
<pre tabindex="0"><code>- LayerNorm layer 1
- self attention layer
- LayerNorm layer 2
- MLP block
</code></pre><p>The flow / forward pass of the input matrix <code>(batch_size B, sequence_length T, hidden_size C)</code> is as follows:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>x <span style="color:#f92672">=</span> x <span style="color:#f92672">+</span> attn(ln1(x))
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> x <span style="color:#f92672">+</span> mlp(ln2(x))
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">return</span> x
</span></span></code></pre></div><p>The shape of the LayerNorm layer 1 is <code>(hidden_size C)</code>, for each batch_size, we apply the layer normalization for each token vector.
The output shape remains the same <code>(batch_size B, sequence_length T, hidden_size C)</code>.</p>
<h3 id="attention-block">Attention Block</h3>
<p>Attention Block is the following:</p>
<pre tabindex="0"><code>- query layer: Linear layer (hidden_size C, hidden_size C)
- key layer: Linear layer (hidden_size C, hidden_size C)
- value layer: Linear layer (hidden_size C, hidden_size C)
- c_proj: Linear layer (hidden_size C, hidden_size C) 
- mask layer: lower triangle 1 matrix (1, 1, sequence length T, sequence_length T)
</code></pre><p>The flow / forward pass on the input matrix <code>(batch_size B, sequence_length T, hidden_size C)</code> is as follows:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 1. Calculate Q, K, V with the separate linear layers</span>
</span></span><span style="display:flex;"><span>q <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>query(x) <span style="color:#75715e"># (B, T, C)</span>
</span></span><span style="display:flex;"><span>k <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>key(x)   <span style="color:#75715e"># (B, T, C)</span>
</span></span><span style="display:flex;"><span>v <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>value(x) <span style="color:#75715e"># (B, T, C)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 2. Split heads and reshape</span>
</span></span><span style="display:flex;"><span>head_size <span style="color:#f92672">=</span> C <span style="color:#f92672">//</span> self<span style="color:#f92672">.</span>n_head
</span></span><span style="display:flex;"><span>q <span style="color:#f92672">=</span> q<span style="color:#f92672">.</span>view(B, T, self<span style="color:#f92672">.</span>n_head, head_size)<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)  <span style="color:#75715e"># (B, nh, T, hs)</span>
</span></span><span style="display:flex;"><span>k <span style="color:#f92672">=</span> k<span style="color:#f92672">.</span>view(B, T, self<span style="color:#f92672">.</span>n_head, head_size)<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)  <span style="color:#75715e"># (B, nh, T, hs)</span>
</span></span><span style="display:flex;"><span>v <span style="color:#f92672">=</span> v<span style="color:#f92672">.</span>view(B, T, self<span style="color:#f92672">.</span>n_head, head_size)<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)  <span style="color:#75715e"># (B, nh, T, hs)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 3. Compute attention scores</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># (B, nh, T, hs) @ (B, nh, hs, T) -&gt; (B, nh, T, T)</span>
</span></span><span style="display:flex;"><span>att <span style="color:#f92672">=</span> (q <span style="color:#f92672">@</span> k<span style="color:#f92672">.</span>transpose(<span style="color:#f92672">-</span><span style="color:#ae81ff">2</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)) <span style="color:#f92672">*</span> (<span style="color:#ae81ff">1.0</span> <span style="color:#f92672">/</span> math<span style="color:#f92672">.</span>sqrt(head_size))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 4. Apply causal mask</span>
</span></span><span style="display:flex;"><span>att <span style="color:#f92672">=</span> att<span style="color:#f92672">.</span>masked_fill(self<span style="color:#f92672">.</span>mask[<span style="color:#66d9ef">None</span>,<span style="color:#66d9ef">None</span>,:T,:T] <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>, float(<span style="color:#e6db74">&#39;-inf&#39;</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 5. Softmax and apply to values</span>
</span></span><span style="display:flex;"><span>att <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>softmax(att, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)  <span style="color:#75715e"># (B, nh, T, T)</span>
</span></span><span style="display:flex;"><span>out <span style="color:#f92672">=</span> att <span style="color:#f92672">@</span> v  <span style="color:#75715e"># (B, nh, T, hs)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 6. Reshape back and project</span>
</span></span><span style="display:flex;"><span>out <span style="color:#f92672">=</span> out<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>contiguous()<span style="color:#f92672">.</span>view(B, T, C)  <span style="color:#75715e"># (B, T, C)</span>
</span></span><span style="display:flex;"><span>out <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>c_proj(out)  <span style="color:#75715e"># (B, T, C)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">return</span> out
</span></span></code></pre></div><p>We first create 3 linear layers (matrices) of query, key and value:</p>
<ul>
<li><strong>Query matrix</strong>: represents the tokens we want to calculate how much other tokens affect</li>
<li><strong>Key matrix</strong>: represents the tokens that affect the query token</li>
<li><strong>Value matrix</strong>: is the actual value of the token</li>
</ul>
<p>These are the 3 different representations that we want to create from our input matrix <code>(batch_size B, sequence_length T, hidden_size C)</code>.
Now we want to create multiple heads of attention that look at different parts of hidden_size.</p>
<p>The idea behind this is that if we use one attention head that looks at all the features of token (meaning it&rsquo;s of hidden_size C),
it can focus only on one aspect/view and ignore other views of how tokens can relate to each other.</p>
<p>So we create multiple attention heads, each head looks at a subset of the token&rsquo;s features. With these we can capture different relations between
tokens and each head can specialize in different types of relationships.</p>
<p>This is why we compute the head size by dividing the hidden size C by the number of heads:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>head_size <span style="color:#f92672">=</span> C <span style="color:#f92672">//</span> self<span style="color:#f92672">.</span>n_head 
</span></span></code></pre></div><p>Now we want to reflect that we want multiple heads, each head will look at the sequence T and each sequence has tokens of embedding size head_size hs now:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>q <span style="color:#f92672">=</span> q<span style="color:#f92672">.</span>view(B, T, self<span style="color:#f92672">.</span>n_head, head_size)<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)  <span style="color:#75715e"># (B, nh, T, hs)</span>
</span></span><span style="display:flex;"><span>k <span style="color:#f92672">=</span> k<span style="color:#f92672">.</span>view(B, T, self<span style="color:#f92672">.</span>n_head, head_size)<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)  <span style="color:#75715e"># (B, nh, T, hs)</span>
</span></span><span style="display:flex;"><span>v <span style="color:#f92672">=</span> v<span style="color:#f92672">.</span>view(B, T, self<span style="color:#f92672">.</span>n_head, head_size)<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)  <span style="color:#75715e"># (B, nh, T, hs)</span>
</span></span></code></pre></div><p>The result of the view is matrices of size <code>(batch_size B, sequence_length T, number of head nh, head_size hs)</code>.
We also do a transpose of dimensions 1 and 2 preparing for the matrix multiplication for each head,
so that the resulting dimension is <code>(batch_size B, number of head nh, sequence_length T, head_size hd)</code>.</p>
<p>Now we compute the attention between different tokens for all the heads:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 3. Compute attention scores</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># (B, nh, T, hs) @ (B, nh, hs, T) -&gt; (B, nh, T, T)</span>
</span></span><span style="display:flex;"><span>att <span style="color:#f92672">=</span> (q <span style="color:#f92672">@</span> k<span style="color:#f92672">.</span>transpose(<span style="color:#f92672">-</span><span style="color:#ae81ff">2</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)) <span style="color:#f92672">*</span> (<span style="color:#ae81ff">1.0</span> <span style="color:#f92672">/</span> math<span style="color:#f92672">.</span>sqrt(head_size))
</span></span></code></pre></div><p>Resulting matrix is of shape <code>(batch_size B, number of heads nh, sequence_length T, sequence_length T)</code>.
Then we apply the causal mask because we don&rsquo;t want tokens to attend to tokens from the future:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 4. Apply causal mask</span>
</span></span><span style="display:flex;"><span>att <span style="color:#f92672">=</span> att<span style="color:#f92672">.</span>masked_fill(self<span style="color:#f92672">.</span>mask[<span style="color:#66d9ef">None</span>,<span style="color:#66d9ef">None</span>,:T,:T] <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>, float(<span style="color:#e6db74">&#39;-inf&#39;</span>))
</span></span></code></pre></div><p>Resulting matrix after masking: <code>(batch_size B, number of heads nh, sequence_length T, sequence_length T)</code>.</p>
<p>At this point the numbers don&rsquo;t add up to one and they are raw similarity scores, this is why we want to apply the softmax
transformation to put the numbers into a normalized probability distribution.
All the values will be between 0 and 1, and the sum across each row (sequence T) will add up to 1.</p>
<p>Resulting matrix after softmax is of shape <code>(batch_size B, number of heads nh, sequence_length T, sequence_length T)</code>.</p>
<p>Now we matrix multiply this importance distribution with the actual values matrix:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 5. Softmax and apply to values</span>
</span></span><span style="display:flex;"><span>att <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>softmax(att, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)  <span style="color:#75715e"># (B, nh, T, T)</span>
</span></span><span style="display:flex;"><span>out <span style="color:#f92672">=</span> att <span style="color:#f92672">@</span> v  <span style="color:#75715e"># (B, nh, T, hs)</span>
</span></span></code></pre></div><p>Resulting matrix after softmax has the shape <code>(batch size B, number of head nh, sequence length T, sequence length T)</code>.
Then the resulting matrix after value multiplication is <code>(batch_size B, number of head nh, sequence length T, head size hs)</code>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 6. Reshape back and project</span>
</span></span><span style="display:flex;"><span>out <span style="color:#f92672">=</span> out<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>contiguous()<span style="color:#f92672">.</span>view(B, T, C)  <span style="color:#75715e"># (B, T, C)</span>
</span></span><span style="display:flex;"><span>out <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>c_proj(out)  <span style="color:#75715e"># (B, T, C)</span>
</span></span></code></pre></div><p>Now that we have the outputs from the attention mechanism from different heads <code>(batch_size B, num_heads nh, sequence_length T, head_size hs)</code>,
we need to reshape it again to combine the results from different heads. That&rsquo;s why we transpose positions 1 and 2 resulting in a matrix with
the following shape <code>(batch_size B, sequence_length T, num_heads nh, head_size hs)</code>. Now we combine the results from the different heads
ending up with a matrix of size <code>(batch_size B, sequence_length T, hidden_size C)</code>.</p>
<p>We then pass it through a linear layer to learn a combined coherent features from the resulting separate features of the attention mechanism
coming from different heads.</p>
<h3 id="mlp-block">MLP Block</h3>
<p>Now we need to define the MLP block:</p>
<pre tabindex="0"><code>- c_fc: Linear layer (hidden_size C, hidden_size C * 4)
- gelu: GeLU activation layer (gelu with tanh activation)
- c_proj: Another Linear layer (hidden_size C * 4, hidden_size C)
</code></pre><p>The flow or the forward pass of the input matrix <code>(batch_size B, sequence_length T, hidden_size C)</code> through the MLP block is as follows:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>c_fc(x)   <span style="color:#75715e"># (B, T, 4 * C)</span>
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>gelu(x)   <span style="color:#75715e"># (B, T, 4 * C)</span>
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>c_proj(x) <span style="color:#75715e"># (B, T, C) </span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">return</span> x
</span></span></code></pre></div><p>The idea behind the first layer c_fc is to project into a higher dimension (from hidden_size C to 4 * hidden_size C) space
to learn more features. Then we apply non-linear GELU transformation, it softly gates input values, values closer to zero
are shrunk more smoothly rather than being zeroed out.</p>
<p>Second linear layer reduces the dimensionality back to hidden_size C from 4 * hidden_size C so now we end up again with a matrix of shape
<code>(batch_size B, sequence_length T, hidden_size C)</code> and this output gets into the next block.</p>
<h2 id="6-final-layers">6. Final Layers</h2>
<p>After we&rsquo;re done with N blocks, we have two final layers:</p>
<pre tabindex="0"><code>- ln_f: Layer norm 
- lm_head: Linear layer (hidden_size C, vocab_size V)
</code></pre><p>This layer will project the embeddings back to vocab_size, now we have logits for each one of the tokens they are not probabilities just yet (we need to add softmax).
We also set this layer with bias=False as biases are unnecessary at this stage and removing them saves memory.</p>
<p>We also introduce the idea of <code>weight sharing / tying</code> which basically consists of making the token embedding layer and the lm_head layer essentially the same.</p>
<p>If we look at matrices shapes, we see that the input embedding layer has shape of <code>(vocab_size V, hidden_size C)</code> but the output projection layer or lm_head
has the shape <code>(hidden_size C, vocab_size V)</code>. By doing the following weight tying:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>self<span style="color:#f92672">.</span>transformer<span style="color:#f92672">.</span>wte<span style="color:#f92672">.</span>weight <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>lm_head<span style="color:#f92672">.</span>weight 
</span></span></code></pre></div><p>We&rsquo;re essentially modifying the pointer for the matrix <code>wte.weight</code> to now point to the same memory address as <code>lm_head.weight</code>. At a first glance I thought
this would result in a shape mismatch error when doing the forward pass, but it doesn&rsquo;t happen because <code>nn.Linear layer</code> does the transpose operation behind the scenes.</p>
<p>The intuition behind doing this &ldquo;weight tying&rdquo; is that both of these layers go to/from embeddings to/from vocab_size. So they either create embeddings for each token or create logits for each token from the embeddings. The way that tokens are represented internally needs to align with the way the model
predicts the tokens. This has many benefits such as you get 1 layer instead of 2 separate layers which means less storage space and more speed.</p>
<h2 id="forward-pass">Forward Pass</h2>
<p>This is how the whole forward pass looks like in Python:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, idx, targets<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>):
</span></span><span style="display:flex;"><span>    _, T <span style="color:#f92672">=</span> idx<span style="color:#f92672">.</span>size()
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">assert</span> T <span style="color:#f92672">&lt;=</span> self<span style="color:#f92672">.</span>config<span style="color:#f92672">.</span>block_size, <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Sequence of length </span><span style="color:#e6db74">{</span>T<span style="color:#e6db74">}</span><span style="color:#e6db74"> should not exceed max length </span><span style="color:#e6db74">{</span>self<span style="color:#f92672">.</span>config<span style="color:#f92672">.</span>block_size<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>
</span></span><span style="display:flex;"><span>    pos <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">0</span>, T, dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>long, device<span style="color:#f92672">=</span>idx<span style="color:#f92672">.</span>device)
</span></span><span style="display:flex;"><span>    pos_embd <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>transformer<span style="color:#f92672">.</span>wpe(pos)
</span></span><span style="display:flex;"><span>    token_embd <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>transformer<span style="color:#f92672">.</span>wte(idx)
</span></span><span style="display:flex;"><span>    x <span style="color:#f92672">=</span> pos_embd <span style="color:#f92672">+</span> token_embd 
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> block <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>transformer<span style="color:#f92672">.</span>h:
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> block(x)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>transformer<span style="color:#f92672">.</span>ln_f(x)
</span></span><span style="display:flex;"><span>    logits <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>lm_head(x)
</span></span><span style="display:flex;"><span>    loss <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span> 
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> targets:
</span></span><span style="display:flex;"><span>        logits_flat <span style="color:#f92672">=</span> logits<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, logits<span style="color:#f92672">.</span>size(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>))  <span style="color:#75715e"># flatten out the logits to become of shape (B*T, vocab_size)</span>
</span></span><span style="display:flex;"><span>        targets_flat <span style="color:#f92672">=</span> targets<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)  <span style="color:#75715e"># flatten out the targets to become of shape (B*T,)  </span>
</span></span><span style="display:flex;"><span>        loss <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>cross_entropy(logits_flat, targets_flat) 
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> logits, loss 
</span></span></code></pre></div><p>The <code>pos</code> variable coming from the torch tensor <code>torch.arange(0, T, dtype=torch.long, device=idx.device)</code>
is basically the PyTorch implementation of <code>list(range(0, T))</code> so that we have a tensor that contains numbers from <code>0</code>
to <code>T</code>. We pass this tensor later on <code>wpe</code> (word position embedding), so that we have an embedding for each position between <code>0</code>
and <code>T</code>.</p>
<p>We pass the input tensor <code>idx</code> which is of shape <code>(batch_size B, sequence_length T)</code> to <code>wte</code> to create embeddings for each input token
and then add them together:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>x <span style="color:#f92672">=</span> pos_embd <span style="color:#f92672">+</span> token_embd
</span></span></code></pre></div><p>We later have to flatten out the logits which are of shape <code>(batch_size B, sequence_length T, vocab_size V)</code> and the targets <code>(batch_size B, sequence_length T)</code>.</p>
<p>PyTorch&rsquo;s <code>F.cross_entropy</code> expects:</p>
<ul>
<li>Predictions: (N, C) where N = number of samples, C = number of classes</li>
<li>Targets: (N,) a 1D tensor of class indices</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>logits_flat <span style="color:#f92672">=</span> logits<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, logits<span style="color:#f92672">.</span>size(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span>targets_flat <span style="color:#f92672">=</span> targets<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
</span></span></code></pre></div><p>Then we calculate the cross entropy <code>cross_entropy</code> loss.</p>

        </section>
        <div class="post-tags">
          
          
          <nav class="nav tags">
            <ul class="tags">
              
              <li><a href="/tags/gpt">gpt</a></li>
              
              <li><a href="/tags/dummies">dummies</a></li>
              
            </ul>
          </nav>
          
          
        </div>
      </div>

      
      
    </div>

    </article>
</main>
<footer>
  <div style="display:flex"><a class="soc" href="https://github.com/azayz" rel="me" title="GitHub"><svg class="feather">
   <use href="/svg/feather-sprite.51cf5647cb1987f769b616558f2620fd9423d72058490231b391bf6aa3744b55.svg#github" />
</svg></a><a class="border"></a><a class="soc" href="https://twitter.com/ABelaweid" rel="me" title="Twitter"><svg class="feather">
   <use href="/svg/feather-sprite.51cf5647cb1987f769b616558f2620fd9423d72058490231b391bf6aa3744b55.svg#twitter" />
</svg></a><a class="border"></a></div>
  <div class="footer-info">
    2025  <a
      href="https://github.com/athul/archie">Archie Theme</a> | Built with <a href="https://gohugo.io">Hugo</a>
  </div>
</footer>

</div>
    </body>
</html>
